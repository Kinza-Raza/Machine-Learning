{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Capture12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images are made of pixels. Black and White images are represented as 2D arrays. Where each pixel holds a value in the range of 0-255 (greyscale) to represent the intensity of colour. 0 is black, 255 is white.\n",
    "\n",
    "![title](Capture13.png)\n",
    "\n",
    "### The coloured images are represented in 3D array. Where each pixel holds a value in the range of 0-255 to represent colour. \n",
    "\n",
    "![title](Capture14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps of CNN:\n",
    "\n",
    "1) Convolution \n",
    "\n",
    "2) Max Pooling\n",
    "\n",
    "3) Flattening \n",
    "\n",
    "4) Full Connection \n",
    "\n",
    "![title](qwer.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Convolution:\n",
    "\n",
    "Kernel, Filter or Feature Detector which is typically a 3x3 matrix, but can be of any dimension. The Filter is multiplied with the image pixels from left to right (traverse). The values are multiplied and summed together. The purpose is to reduce the size of image. Usually stride length is 1. If it is 2, the dimensions with be further reduced. Increasing stride length may lead to loss of information. \n",
    "The result is the \"Convuled Feature\" or \"Activation Map\" or \"Feature Map\"\n",
    "\n",
    "Feature map helps to preserve the special features that help us in distinguishing different objects. This forms our convolution layer. \n",
    "\n",
    "We make multiple feature maps by applying different filters in order to prevent loss of information.\n",
    "\n",
    "The first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.\n",
    "\n",
    "![title](Capture15.png)\n",
    "\n",
    "#### Example of applying feature detector on input image: \n",
    "\n",
    "![title](Capture16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
